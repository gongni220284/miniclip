{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d31af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import io\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, List, Optional, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73f76b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flicker_dir = Path(\"../data/flicker\")\n",
    "raw_data_dir = Path(\"../data/raw_dataset\")\n",
    "flicker_dir.mkdir(parents=True, exist_ok=True)\n",
    "raw_data_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6226df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting...\n",
      "\n",
      "TÃ©lÃ©chargement rÃ©ussi. Extraction...\n",
      "DonnÃ©es extraites dans : ../data/flicker\n"
     ]
    }
   ],
   "source": [
    "# '''Get dataset from flicker'''\n",
    "# url = \"https://www.lirmm.fr/~poncelet/Ressources/flickr_subset2.zip\"\n",
    "# print(\"Requesting...\\n\")\n",
    "# response = requests.get(url)\n",
    "# if response.status_code == 200:\n",
    "#     print(\"TÃ©lÃ©chargement rÃ©ussi. Extraction...\")\n",
    "#     with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "#         # Extraire sans ajouter de sous-dossier supplÃ©mentaire\n",
    "#         for member in zip_ref.namelist():\n",
    "#             # Corrige les chemins pour ignorer un Ã©ventuel prefixe flickr_subset2/\n",
    "#             member_path = member\n",
    "#             if member.startswith(\"flickr_subset2/\"):\n",
    "#                 member_path = member[len(\"flickr_subset2/\"):]\n",
    "#             target_path = flicker_dir / member_path\n",
    "\n",
    "#             # Si c'est un rÃ©pertoire, on le crÃ©e\n",
    "#             if member.endswith(\"/\"):\n",
    "#                 target_path.mkdir(exist_ok=True, parents=True)\n",
    "#             else:\n",
    "#                 os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "#                 with zip_ref.open(member) as source, open(target_path, \"wb\") as target:\n",
    "#                     target.write(source.read())\n",
    "#     print(f\"DonnÃ©es extraites dans : {flicker_dir}\")\n",
    "# else:\n",
    "#     print(\"Ã‰chec du tÃ©lÃ©chargement. Code HTTP :\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "194c77ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_path', 'label', 'caption'], dtype='object')\n",
      "image_path                        images/dog/dog_001.jpg\n",
      "label                                                dog\n",
      "caption       A black dog and a spotted dog are fighting\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "caption_csv_path = flicker_dir / \"captions.csv\"\n",
    "df = pd.read_csv(caption_csv_path)\n",
    "print(df.columns)\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "09b1ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Tuple\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((300, 500)),\n",
    "        transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "class ImageCLIPDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, flicker_dir, transform):\n",
    "        self.img_paths = [Path(flicker_dir / img) for img in imgs]\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(set(labels))                  # toutes les classes uniques\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}  # mapping texte -> int\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[Image.Image, str]:\n",
    "        img = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
    "        label_str = self.labels[idx]\n",
    "        label = self.class_to_idx[label_str]   # âœ… convertit en entier\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return idx, img, label\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def _get_img_path_from_idx(self, idx: int) -> Path:\n",
    "        return self.img_paths[idx]\n",
    "    \n",
    "    def _get_label_from_idx(self, idx: int) -> str :\n",
    "        return self.labels[idx]\n",
    "    \n",
    "    def _get_img_size(self, idx: int) -> Tuple[int, int]:\n",
    "        img = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            for t in self.transform.transforms:\n",
    "                if isinstance(t, transforms.Resize):\n",
    "                    img = t(img)\n",
    "        return img.height, img.width\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "706b4eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = df[\"image_path\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "train_dataset = ImageCLIPDataset(X_train, y_train, flicker_dir, transform)\n",
    "test_dataset = ImageCLIPDataset(X_test, y_test, flicker_dir, transform)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1a9d12a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 500)\n",
      "(300, 500)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset._get_img_size(3))\n",
    "print(train_dataset._get_img_size(13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b6767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d816ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNBasic(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(CNNBasic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.flattened_size = self._get_flattened_size()\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _get_flattened_size(self):\n",
    "        # on simule une image dâ€™entrÃ©e (300x500 comme ton transform)\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(1, 3, 300, 500)\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            return x.view(1, -1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "75902df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNBasic(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "337e8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d67fb384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "08ecc9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "480it [03:50,  2.08it/s]\n",
      "480it [05:43,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "for epoch in range(2):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(train_loader, 0)):\n",
    "        idx, inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d5bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4340, -0.2184, -0.0767, -0.3644]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "02fb63c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 480/480 [00:25<00:00, 18.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š --- RÃ©sultats finaux ---\n",
      "Loss moyenne : 1.1340\n",
      "PrÃ©cision (accuracy) : 56.04%\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Met le modÃ¨le en mode Ã©valuation\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "losses = []\n",
    "\n",
    "# DÃ©sactive le calcul du gradient pour lâ€™Ã©valuation\n",
    "with torch.no_grad():\n",
    "    for idx, inputs, labels in tqdm(train_loader, desc=\"Evaluation\"):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # prÃ©dictions\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Moyenne des pertes et prÃ©cision\n",
    "avg_loss = np.mean(losses)\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(\"\\nðŸ“Š --- RÃ©sultats finaux ---\")\n",
    "print(f\"Loss moyenne : {avg_loss:.4f}\")\n",
    "print(f\"PrÃ©cision (accuracy) : {accuracy:.2f}%\")\n",
    "print(\"-----------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f9c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
